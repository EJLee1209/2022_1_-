{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 퍼셉트론의 원리\n",
    "> 퍼셉트론은 입력측과 출력측으로 구성된다. 입력층에는 d+1개의 노드가 있다.(d는 특징 벡터의 차원)\n",
    "\n",
    "> 입력층의 i번째 노드와 출력층의 노드는 가중치 w를 갖는 에지로 연결된다. i번째 에지는 특징 x 와 가중치 w를 곱해 출력노드로 전달한다.\n",
    "\n",
    "> 0번째 노드의 입력 x는 항상 1인데, 이 노드를 바이어스 노드(bias)라 부른다.\n",
    "\n",
    "> 출력노드는 d+1개의 곱셈 결과를 모두 더한 s를 계산한다. 그리고 s를 활성 함수에 적용한 결과를 출력 o로 내보낸다.\n",
    "\n",
    "> s > 0  -->  1\n",
    "\n",
    "> s <= 0 --> -1\n",
    "\n",
    "# 퍼셉트론 학습 알고리즘\n",
    "> 퍼셉트론을 학습하려면 손실함수 J를 설계해야하며 손실함수 값을 낮추는 방향을 찾아야한다.\n",
    "\n",
    "## 손실 함수 설계\n",
    "1. w가 훈련 집합에 있는 샘플을 모두 맞히면, 즉 정활률이 100%이면 J(w)(손실함수)는 0이다.\n",
    "2. w가 틀리는 샘플이 많을수록 J(w)의 값이 크다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습된 퍼셉트론의 매개변수:  [[2. 2.]] [-1.]\n",
      "훈련 집합에 대한 예측:  [-1  1  1  1]\n",
      "정확률 측정:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "X=[[0,0],[0,1],[1,0],[1,1]]\n",
    "y=[-1,1,1,1]\n",
    "\n",
    "p=Perceptron() #객체 생성\n",
    "p.fit(X,y) # 학습\n",
    "\n",
    "print(\"학습된 퍼셉트론의 매개변수: \",p.coef_,p.intercept_) # p.coef_ : 가중치1~가중치2 intercept_ : 가중치0\n",
    "print(\"훈련 집합에 대한 예측: \",p.predict(X))\n",
    "print(\"정확률 측정: \",p.score(X,y)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필기 숫자 데이터 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "digit=datasets.load_digits()\n",
    "x_train,x_test,y_train,y_test = train_test_split(digit.data,digit.target,train_size=0.6) # 훈련/테스트 집합으로 분할\n",
    "\n",
    "p=Perceptron(max_iter=100,eta0=0.001,verbose=0)# Perceptron객체 생성\n",
    "p.fit(x_train, y_train) # 학습\n",
    "\n",
    "res = p.predict(x_test) # 예측\n",
    "\n",
    "conf = np.zeros((10,10)) # 혼동행렬\n",
    "for i in range(len(res)):\n",
    "    conf[res[i]][y_test[i]]+=1\n",
    "print(conf)\n",
    "\n",
    "no_correct = 0 # 맞은 샘플의 수\n",
    "for i in range(10):\n",
    "    no_correct += conf[i][i]\n",
    "accuracy = no_correct/len(res) # 정확률 계산\n",
    "print(\"테스트 집합에 대한 정확률은 \", accuracy*100,\"%입니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기계 학습의 디자인 패턴\n",
    "> 의미 없는 일의 반복을 뜻하는 '바퀴의 재발명'을 피하려면 어느 분야든지 패턴을 잘 활용해야 한다.\n",
    "* sklearn의 디자인 패턴\n",
    "    \n",
    "    데이터 읽기 --> 모델 객체 생성 --> 모델 학습 --> 예측 --> 성능평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다층 퍼셉트론\n",
    "퍼셉트론은 선형 분류기라는 근본적인 한계가 있어서 선형 분리가 불가능한 상황에서 낮은 성능을 보인다.\n",
    "그 한계를 극복하기 위한 것이 바로 다층 퍼셉트론이다.\n",
    "\n",
    "## 특징 공간 변환\n",
    "퍼셉트론을 여러개 사용하면 새로운 특징 공간을 만들어 원래 공간에서는 선형 분리가 불가능하던 것을 가능하게 만든다.\n",
    "\n",
    "## 다층 퍼셉트론의 구조\n",
    "> 다층 퍼셉트론은 입력층, 은닉층, 출력층으로 구성된다.\n",
    "\n",
    "> 데이터가 주어지면 입력층의 노드 수와 출력층의 노드 수는 자동으로 정해진다.\n",
    "\n",
    "> 은닉층의 노드 개수는 하이퍼 매개변수이다. 은닉층의 노드 수가 많으면 신경망의 용량이 커져서 복잡한 데이터를 모델링하는데는 유리하지만, 학습이 오래걸리고 과잉적합이 나타날 가능성이 크다\n",
    "\n",
    "# 오류 역전파 알고리즘\n",
    "> 다층 퍼셉트론을 학습하려면 퍼셉트론과 마찬가지로 손실함수를 정의하고 손실 함수의 최적점을 찾는 최적화 알고리즘을 고안해야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다층 퍼셉트론 프로그래밍\n",
    "## sklearn의 필기 숫자 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.17161661\n",
      "Iteration 2, loss = 0.33034530\n",
      "Iteration 3, loss = 0.21029193\n",
      "Iteration 4, loss = 0.16289602\n",
      "Iteration 5, loss = 0.14155698\n",
      "Iteration 6, loss = 0.11708756\n",
      "Iteration 7, loss = 0.09953593\n",
      "Iteration 8, loss = 0.08528539\n",
      "Iteration 9, loss = 0.08349285\n",
      "Iteration 10, loss = 0.07248814\n",
      "Iteration 11, loss = 0.06477120\n",
      "Iteration 12, loss = 0.06049482\n",
      "Iteration 13, loss = 0.05539430\n",
      "Iteration 14, loss = 0.05102397\n",
      "Iteration 15, loss = 0.04828605\n",
      "Iteration 16, loss = 0.04767732\n",
      "Iteration 17, loss = 0.04152829\n",
      "Iteration 18, loss = 0.04007246\n",
      "Iteration 19, loss = 0.03835793\n",
      "Iteration 20, loss = 0.03475521\n",
      "Iteration 21, loss = 0.03396819\n",
      "Iteration 22, loss = 0.03552277\n",
      "Iteration 23, loss = 0.03085171\n",
      "Iteration 24, loss = 0.02842405\n",
      "Iteration 25, loss = 0.02741303\n",
      "Iteration 26, loss = 0.02678203\n",
      "Iteration 27, loss = 0.02551091\n",
      "Iteration 28, loss = 0.02526601\n",
      "Iteration 29, loss = 0.02330905\n",
      "Iteration 30, loss = 0.02247823\n",
      "Iteration 31, loss = 0.02222515\n",
      "Iteration 32, loss = 0.02169266\n",
      "Iteration 33, loss = 0.02086210\n",
      "Iteration 34, loss = 0.01977269\n",
      "Iteration 35, loss = 0.01960007\n",
      "Iteration 36, loss = 0.01872088\n",
      "Iteration 37, loss = 0.01803278\n",
      "Iteration 38, loss = 0.01719225\n",
      "Iteration 39, loss = 0.01618587\n",
      "Iteration 40, loss = 0.01578927\n",
      "Iteration 41, loss = 0.01608161\n",
      "Iteration 42, loss = 0.01511761\n",
      "Iteration 43, loss = 0.01506821\n",
      "Iteration 44, loss = 0.01482331\n",
      "Iteration 45, loss = 0.01398917\n",
      "Iteration 46, loss = 0.01348031\n",
      "Iteration 47, loss = 0.01326404\n",
      "Iteration 48, loss = 0.01313736\n",
      "Iteration 49, loss = 0.01257119\n",
      "Iteration 50, loss = 0.01239922\n",
      "Iteration 51, loss = 0.01186904\n",
      "Iteration 52, loss = 0.01162449\n",
      "Iteration 53, loss = 0.01127548\n",
      "Iteration 54, loss = 0.01137922\n",
      "Iteration 55, loss = 0.01110921\n",
      "Iteration 56, loss = 0.01051254\n",
      "Iteration 57, loss = 0.01047972\n",
      "Iteration 58, loss = 0.01030926\n",
      "Iteration 59, loss = 0.01000198\n",
      "Iteration 60, loss = 0.00995007\n",
      "Iteration 61, loss = 0.00974312\n",
      "Iteration 62, loss = 0.00950577\n",
      "Iteration 63, loss = 0.00930721\n",
      "Iteration 64, loss = 0.00912210\n",
      "Iteration 65, loss = 0.00890484\n",
      "Iteration 66, loss = 0.00898857\n",
      "Iteration 67, loss = 0.00876683\n",
      "Iteration 68, loss = 0.00854489\n",
      "Iteration 69, loss = 0.00835313\n",
      "Iteration 70, loss = 0.00822041\n",
      "Iteration 71, loss = 0.00820190\n",
      "Iteration 72, loss = 0.00806449\n",
      "Iteration 73, loss = 0.00783741\n",
      "Iteration 74, loss = 0.00772437\n",
      "Iteration 75, loss = 0.00763127\n",
      "Iteration 76, loss = 0.00749898\n",
      "Iteration 77, loss = 0.00745026\n",
      "Iteration 78, loss = 0.00736138\n",
      "Iteration 79, loss = 0.00720679\n",
      "Iteration 80, loss = 0.00711938\n",
      "Iteration 81, loss = 0.00705764\n",
      "Iteration 82, loss = 0.00693931\n",
      "Iteration 83, loss = 0.00674919\n",
      "Iteration 84, loss = 0.00674505\n",
      "Iteration 85, loss = 0.00658314\n",
      "Iteration 86, loss = 0.00646687\n",
      "Iteration 87, loss = 0.00648038\n",
      "Iteration 88, loss = 0.00640566\n",
      "Iteration 89, loss = 0.00631679\n",
      "Iteration 90, loss = 0.00626902\n",
      "Iteration 91, loss = 0.00614371\n",
      "Iteration 92, loss = 0.00606216\n",
      "Iteration 93, loss = 0.00600552\n",
      "Iteration 94, loss = 0.00595556\n",
      "Iteration 95, loss = 0.00587544\n",
      "Iteration 96, loss = 0.00575554\n",
      "Iteration 97, loss = 0.00576969\n",
      "Iteration 98, loss = 0.00562814\n",
      "Iteration 99, loss = 0.00557536\n",
      "Iteration 100, loss = 0.00547020\n",
      "Iteration 101, loss = 0.00543625\n",
      "Iteration 102, loss = 0.00546325\n",
      "Iteration 103, loss = 0.00530438\n",
      "Iteration 104, loss = 0.00525320\n",
      "Iteration 105, loss = 0.00519282\n",
      "Iteration 106, loss = 0.00517161\n",
      "Iteration 107, loss = 0.00508175\n",
      "Iteration 108, loss = 0.00506200\n",
      "Iteration 109, loss = 0.00503852\n",
      "Iteration 110, loss = 0.00494871\n",
      "Iteration 111, loss = 0.00487452\n",
      "Iteration 112, loss = 0.00488602\n",
      "Iteration 113, loss = 0.00487841\n",
      "Iteration 114, loss = 0.00472832\n",
      "Iteration 115, loss = 0.00476677\n",
      "Iteration 116, loss = 0.00463960\n",
      "Iteration 117, loss = 0.00465707\n",
      "Iteration 118, loss = 0.00462255\n",
      "Iteration 119, loss = 0.00462036\n",
      "Iteration 120, loss = 0.00454764\n",
      "Iteration 121, loss = 0.00449301\n",
      "Iteration 122, loss = 0.00452846\n",
      "Iteration 123, loss = 0.00438947\n",
      "Iteration 124, loss = 0.00436695\n",
      "Iteration 125, loss = 0.00432071\n",
      "Iteration 126, loss = 0.00427526\n",
      "Iteration 127, loss = 0.00423188\n",
      "Iteration 128, loss = 0.00422276\n",
      "Iteration 129, loss = 0.00415538\n",
      "Iteration 130, loss = 0.00415038\n",
      "Iteration 131, loss = 0.00409979\n",
      "Iteration 132, loss = 0.00403513\n",
      "Iteration 133, loss = 0.00400149\n",
      "Iteration 134, loss = 0.00399537\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[[70.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. 77.  0.  0.  0.  0.  1.  1.  3.  0.]\n",
      " [ 0.  0. 67.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. 74.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0. 72.  0.  0.  1.  1.  0.]\n",
      " [ 2.  0.  0.  0.  0. 80.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1. 61.  0.  0.  0.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0. 61.  0.  0.]\n",
      " [ 0.  2.  0.  0.  0.  0.  0.  0. 68.  0.]\n",
      " [ 0.  0.  0.  2.  1.  1.  0.  0.  0. 69.]]\n",
      "테스트 집합에 대한 정확률은  97.2183588317107 %입니다.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "digit=datasets.load_digits()\n",
    "x_train, x_test, y_train, y_test=train_test_split(digit.data,digit.target,train_size=0.6) # 훈련/테스트집합 분할\n",
    "\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(100),learning_rate_init=0.001,batch_size=32,max_iter=300,solver='sgd',verbose=True)\n",
    "# hidden_layer_sizes : 은닉층의 크기\n",
    "# learning_rate_init : 학습률 초깃값\n",
    "# batch_size : 배치 크기\n",
    "# max_iter : 에포크 횟수(최대 몇번 돌릴건지)\n",
    "# solver : 경사 하강법 알고리즘의 종류 (sgd : stochastic gradient descent)\n",
    "# verbose : 진행 메시지 출력 여부\n",
    "mlp.fit(x_train,y_train) #학습\n",
    "\n",
    "res=mlp.predict(x_test) #예측\n",
    "\n",
    "#혼동행렬\n",
    "conf=np.zeros((10,10))\n",
    "for i in range(len(res)):\n",
    "    conf[res[i]][y_test[i]]+=1\n",
    "print(conf)\n",
    "\n",
    "#정확률 계산\n",
    "no_correct=0\n",
    "for i in range(10):\n",
    "    no_correct+=conf[i][i]\n",
    "accuracy=no_correct/len(res)\n",
    "print(\"테스트 집합에 대한 정확률은 \",accuracy*100,\"%입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 데이터셋으로 확장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.62363114\n",
      "Iteration 2, loss = 0.26275690\n",
      "Iteration 3, loss = 0.20485543\n",
      "Iteration 4, loss = 0.16943145\n",
      "Iteration 5, loss = 0.14498372\n",
      "Iteration 6, loss = 0.12619363\n",
      "Iteration 7, loss = 0.11212430\n",
      "Iteration 8, loss = 0.10090834\n",
      "Iteration 9, loss = 0.09042812\n",
      "Iteration 10, loss = 0.08244439\n",
      "Iteration 11, loss = 0.07524623\n",
      "Iteration 12, loss = 0.06969704\n",
      "Iteration 13, loss = 0.06306259\n",
      "Iteration 14, loss = 0.05960088\n",
      "Iteration 15, loss = 0.05385117\n",
      "Iteration 16, loss = 0.05083298\n",
      "Iteration 17, loss = 0.04695217\n",
      "Iteration 18, loss = 0.04344702\n",
      "Iteration 19, loss = 0.04025003\n",
      "Iteration 20, loss = 0.03781198\n",
      "Iteration 21, loss = 0.03489200\n",
      "Iteration 22, loss = 0.03212986\n",
      "Iteration 23, loss = 0.02971698\n",
      "Iteration 24, loss = 0.02832562\n",
      "Iteration 25, loss = 0.02611589\n",
      "Iteration 26, loss = 0.02467066\n",
      "Iteration 27, loss = 0.02292186\n",
      "Iteration 28, loss = 0.02114101\n",
      "Iteration 29, loss = 0.01975866\n",
      "Iteration 30, loss = 0.01816454\n",
      "Iteration 31, loss = 0.01723203\n",
      "Iteration 32, loss = 0.01594454\n",
      "Iteration 33, loss = 0.01476357\n",
      "Iteration 34, loss = 0.01416115\n",
      "Iteration 35, loss = 0.01331113\n",
      "Iteration 36, loss = 0.01247142\n",
      "Iteration 37, loss = 0.01159380\n",
      "Iteration 38, loss = 0.01042131\n",
      "Iteration 39, loss = 0.01002810\n",
      "Iteration 40, loss = 0.00920793\n",
      "Iteration 41, loss = 0.00858881\n",
      "Iteration 42, loss = 0.00779485\n",
      "Iteration 43, loss = 0.00739537\n",
      "Iteration 44, loss = 0.00728485\n",
      "Iteration 45, loss = 0.00641411\n",
      "Iteration 46, loss = 0.00587372\n",
      "Iteration 47, loss = 0.00561892\n",
      "Iteration 48, loss = 0.00529077\n",
      "Iteration 49, loss = 0.00494240\n",
      "Iteration 50, loss = 0.00462052\n",
      "Iteration 51, loss = 0.00425213\n",
      "Iteration 52, loss = 0.00408991\n",
      "Iteration 53, loss = 0.00362960\n",
      "Iteration 54, loss = 0.00341599\n",
      "Iteration 55, loss = 0.00319594\n",
      "Iteration 56, loss = 0.00300654\n",
      "Iteration 57, loss = 0.00284071\n",
      "Iteration 58, loss = 0.00287901\n",
      "Iteration 59, loss = 0.00263217\n",
      "Iteration 60, loss = 0.00237136\n",
      "Iteration 61, loss = 0.00233798\n",
      "Iteration 62, loss = 0.00213090\n",
      "Iteration 63, loss = 0.00199854\n",
      "Iteration 64, loss = 0.00200099\n",
      "Iteration 65, loss = 0.00183029\n",
      "Iteration 66, loss = 0.00170400\n",
      "Iteration 67, loss = 0.00157957\n",
      "Iteration 68, loss = 0.00148355\n",
      "Iteration 69, loss = 0.00140613\n",
      "Iteration 70, loss = 0.00135615\n",
      "Iteration 71, loss = 0.00131073\n",
      "Iteration 72, loss = 0.00123725\n",
      "Iteration 73, loss = 0.00114788\n",
      "Iteration 74, loss = 0.00112275\n",
      "Iteration 75, loss = 0.00111583\n",
      "Iteration 76, loss = 0.00107663\n",
      "Iteration 77, loss = 0.00096659\n",
      "Iteration 78, loss = 0.00092570\n",
      "Iteration 79, loss = 0.00088305\n",
      "Iteration 80, loss = 0.00085098\n",
      "Iteration 81, loss = 0.00080623\n",
      "Iteration 82, loss = 0.00085081\n",
      "Iteration 83, loss = 0.00077447\n",
      "Iteration 84, loss = 0.00073753\n",
      "Iteration 85, loss = 0.00068462\n",
      "Iteration 86, loss = 0.00066812\n",
      "Iteration 87, loss = 0.00062500\n",
      "Iteration 88, loss = 0.00061945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[[9.700e+02 0.000e+00 4.000e+00 2.000e+00 1.000e+00 2.000e+00 4.000e+00\n",
      "  0.000e+00 3.000e+00 1.000e+00]\n",
      " [0.000e+00 1.123e+03 2.000e+00 0.000e+00 0.000e+00 1.000e+00 2.000e+00\n",
      "  2.000e+00 0.000e+00 2.000e+00]\n",
      " [0.000e+00 4.000e+00 1.005e+03 3.000e+00 2.000e+00 0.000e+00 3.000e+00\n",
      "  8.000e+00 4.000e+00 0.000e+00]\n",
      " [4.000e+00 1.000e+00 3.000e+00 9.910e+02 1.000e+00 1.200e+01 1.000e+00\n",
      "  5.000e+00 7.000e+00 5.000e+00]\n",
      " [0.000e+00 0.000e+00 1.000e+00 0.000e+00 9.610e+02 1.000e+00 5.000e+00\n",
      "  3.000e+00 4.000e+00 9.000e+00]\n",
      " [0.000e+00 1.000e+00 0.000e+00 4.000e+00 0.000e+00 8.680e+02 3.000e+00\n",
      "  0.000e+00 5.000e+00 5.000e+00]\n",
      " [3.000e+00 3.000e+00 2.000e+00 0.000e+00 5.000e+00 3.000e+00 9.360e+02\n",
      "  1.000e+00 1.000e+00 0.000e+00]\n",
      " [1.000e+00 1.000e+00 8.000e+00 3.000e+00 3.000e+00 1.000e+00 1.000e+00\n",
      "  1.001e+03 5.000e+00 3.000e+00]\n",
      " [2.000e+00 2.000e+00 6.000e+00 3.000e+00 1.000e+00 2.000e+00 3.000e+00\n",
      "  2.000e+00 9.420e+02 4.000e+00]\n",
      " [0.000e+00 0.000e+00 1.000e+00 4.000e+00 8.000e+00 2.000e+00 0.000e+00\n",
      "  6.000e+00 3.000e+00 9.800e+02]]\n",
      "테스트 집합에 대한 정확률은  97.77 %입니다.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# MNIST 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
    "mnist=fetch_openml('mnist_784')\n",
    "mnist.data=mnist.data/255.0\n",
    "x_train=mnist.data[:60000]; x_test=mnist.data[60000:]\n",
    "y_train=np.int16(mnist.target[:60000]); y_test=np.int16(mnist.target[60000:]); \n",
    "\n",
    "# MLP 분류기 모델을 학습\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(100),learning_rate_init=0.001,batch_size=512,max_iter=300,solver=\"adam\",verbose=True)\n",
    "# hidden_layer_sizes : 은닉층의 크기\n",
    "# learning_rate_init : 학습률 초깃값\n",
    "# batch_size : 배치 크기\n",
    "# max_iter : 에포크 횟수(최대 몇번 돌릴건지)\n",
    "# solver : 경사 하강법 알고리즘의 종류\n",
    "# verbose : 진행 메시지 출력 여부\n",
    "mlp.fit(x_train,y_train)\n",
    "\n",
    "# 테스트 집합으로 예측\n",
    "res=mlp.predict(x_test)\n",
    "\n",
    "#혼동행렬\n",
    "conf=np.zeros((10,10))\n",
    "for i in range(len(res)):\n",
    "    conf[res[i]][y_test[i]]+=1\n",
    "print(conf)\n",
    "\n",
    "#정확률 계산\n",
    "no_correct=0\n",
    "for i in range(10):\n",
    "    no_correct+=conf[i][i]\n",
    "accuracy=no_correct/len(res)\n",
    "print(\"테스트 집합에 대한 정확률은 \",accuracy*100,\"%입니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼 매개변수 최적화\n",
    "* hidden_layer_sizes = (100)\n",
    "> 다층 퍼셉트론의 구조를 제어하는 하이퍼 매개변수이다 노드가 100개인 은닉층 하나를 두라는 뜻이다.\n",
    "\n",
    "> 만약 hidden_layer_sizes = (100,80)으로 설정하면 노드가 100개인 은닉층과 80개인 은닉층을 두어 은닉층이 2개인 다층 퍼셉트론이 된다.\n",
    "\n",
    "## 단일 하이퍼 매개변수 최적화: validation_curve 함수 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하이퍼 매개변수 최적화에 걸린시간은  127.3197181224823 초 입니다.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\최적의 은닉층 노드 개수는 750 개 입니다.\n",
      "\n",
      "[[98.  0.  0.  0.  3.  0.  0.  0.  0.  0.]\n",
      " [ 0. 57.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0. 63.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. 75.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. 79.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0. 66.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  4.  0. 63.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0. 55.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1. 64.  1.]\n",
      " [ 0.  0.  0.  0.  0.  2.  0.  0.  0. 80.]]\n",
      "테스트 집합에 대한 정확률은  97.35744089012516 %입니다.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, validation_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
    "digit=datasets.load_digits()\n",
    "x_train,x_test,y_train,y_test=train_test_split(digit.data,digit.target,train_size=0.6)\n",
    "\n",
    "# 다층 퍼셉트론을 교차 검증으로 성능 평가(소요 시간 측정 포함)\n",
    "start=time.time()\n",
    "mlp=MLPClassifier(learning_rate_init=0.001,batch_size=32,max_iter=300,solver='sgd')\n",
    "prange=range(50,1001,50) # 조사할 범위(은닉층의 노드 수 50 ~ 1000 까지 50씩 늘려가면서)\n",
    "\n",
    "# 지정한 범위에 있는 조사점 각각에 대해 학습을 수행하고 성능을 측정\n",
    "# cv=10 10-겹 교차검증\n",
    "# scoring='accuracy' 성능측정을 위해 정확률을 사용\n",
    "# n_jobs=8 코어를 8개를 사용해 병렬 처리 -> 값이 클수록 빠른 처리가능\n",
    "train_score,test_score=validation_curve(mlp,x_train,y_train,param_name=\"hidden_layer_sizes\",param_range=prange,cv=10,scoring=\"accuracy\",n_jobs=8)\n",
    "# train_score, test_score에는 각각의 조사점에 대해 10-겹 교차검증으로 얻은 10개의 정확률을 담고 있다.\n",
    "\n",
    "end=time.time()\n",
    "print(\"하이퍼 매개변수 최적화에 걸린시간은 \",end-start,\"초 입니다.\")\n",
    "\n",
    "# 교차 검증 결과의 평균과 분산 구하기\n",
    "train_mean=np.mean(train_score,axis=1)\n",
    "train_std=np.std(train_score,axis=1)\n",
    "test_mean=np.mean(test_score,axis=1)\n",
    "test_std=np.std(test_score,axis=1)\n",
    "\n",
    "# 성능 그래프 그리기\n",
    "plt.plot(prange,train_mean,label=\"Train score\",color=\"r\")\n",
    "plt.plot(prange,test_mean,label=\"Test score\",color=\"b\")\n",
    "plt.fill_between(prange,train_mean-train_std,train_mean+train_std,alpha=0.2,color=\"r\")\n",
    "plt.fill_between(prange,test_mean-test_std,test_mean+test_std,alpha=0.2,color=\"b\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Validation Curve with MLP\")\n",
    "plt.xlabel(\"Number of hidden nodes\")\n",
    "plt.ylim(0.9,1.01)\n",
    "plt.grid(axis=\"both\")\n",
    "plt.show()\n",
    "\n",
    "best_number_nodes=prange[np.argmax(test_mean)] # 최적의 은닉 노드 수\n",
    "print(\"\\최적의 은닉층 노드 개수는\",best_number_nodes,\"개 입니다.\\n\")\n",
    "\n",
    "# 최적의 은닉 노드 수로 모델링\n",
    "mlp_test=MLPClassifier(hidden_layer_sizes=(best_number_nodes),learning_rate_init=0.001,batch_size=32,max_iter=300,solver='sgd')\n",
    "mlp_test.fit(x_train,y_train)\n",
    "\n",
    "# 테스트 집합으로 예측\n",
    "res=mlp_test.predict(x_test)\n",
    "\n",
    "#혼동행렬\n",
    "conf=np.zeros((10,10))\n",
    "for i in range(len(res)):\n",
    "    conf[res[i]][y_test[i]]+=1\n",
    "print(conf)\n",
    "\n",
    "#정확률 계산\n",
    "no_correct=0\n",
    "for i in range(10):\n",
    "    no_correct+=conf[i][i]\n",
    "accuracy=no_correct/len(res)\n",
    "print(\"테스트 집합에 대한 정확률은 \",accuracy*100,\"%입니다.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
